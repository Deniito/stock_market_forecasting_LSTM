{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T21:34:55.314932Z",
     "start_time": "2025-04-10T21:34:55.307026Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import yfinance as yf\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta_remake as ta\n",
    "from pathlib import Path\n",
    "import concurrent.futures"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:35:00.787188Z",
     "start_time": "2025-04-10T21:35:00.778875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#expected to be a directory with submission, companyfacts, ticker.txt and company_tickers.json all available on SEC website\n",
    "data_dir = \"data\"\n",
    "\n",
    "facts_dir = os.path.join(data_dir, \"facts\")\n",
    "stock_data_dir = os.path.join(data_dir, \"stock_data\")\n",
    "\n",
    "company_data_dir = os.path.join(data_dir, \"company_data\")\n",
    "\n",
    "submissions_dir = os.path.join(data_dir, \"submission\")\n",
    "company_facts_dir = os.path.join(data_dir, \"companyfacts\")"
   ],
   "id": "aa04c73991d1b157",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T21:35:02.500849Z",
     "start_time": "2025-04-10T21:35:02.425842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cik_ticker_cvs = os.path.join(data_dir, \"CIK.csv\")\n",
    "\n",
    "ticker_df = pl.read_csv(cik_ticker_cvs).with_columns(\n",
    "    pl.col(\"cik_str\").cast(pl.Utf8).str.zfill(10))"
   ],
   "id": "429da99431c20f2e",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:03:26.430834Z",
     "start_time": "2025-04-10T22:03:26.388093Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_facts_to_csv(ticker_df, source_dir, output_folder, max_workers=8):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    valid_ciks = set(ticker_df[\"cik_str\"].cast(str))\n",
    "    file_list = [f for f in os.listdir(source_dir) if f.endswith(\".json\")]\n",
    "    file_paths = [os.path.join(source_dir, f) for f in file_list]\n",
    "\n",
    "    def process_file(file_path):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        cik_number = file_name.replace(\"CIK\", \"\").split(\".\")[0]\n",
    "\n",
    "        if cik_number not in valid_ciks:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            output_csv = os.path.join(output_folder, f\"{os.path.splitext(file_name)[0]}.csv\")\n",
    "\n",
    "            facts = data.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "            rows = []\n",
    "\n",
    "            for field, field_data in facts.items():\n",
    "                units = field_data.get(\"units\", {})\n",
    "                for unit_type, entries in units.items():\n",
    "                    for entry in entries:\n",
    "                        entry[\"field\"] = field\n",
    "                        entry[\"unit_type\"] = unit_type\n",
    "                        rows.append(entry)\n",
    "\n",
    "            if rows:\n",
    "                df = pl.DataFrame(rows)\n",
    "                df.write_csv(output_csv)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    # Multithreading with progress bar\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_file, path) for path in file_paths]\n",
    "        for _ in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "            pass\n",
    "    \n",
    "def count_unique_metrics(source_dir, output_csv=\"unique_metrics.csv\"):\n",
    "    metric_counts = Counter()\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            \n",
    "            try:\n",
    "                df = pl.read_csv(file_path)\n",
    "                if \"field\" in df.columns:\n",
    "                    metrics = df[\"field\"].drop_nulls().to_list()\n",
    "                    metric_counts.update(metrics) \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    output_df = pl.DataFrame({\n",
    "        \"metric\": list(metric_counts.keys()),\n",
    "        \"amount\": list(metric_counts.values())\n",
    "    })\n",
    "    output_df.write_csv(output_csv)\n",
    "    print(f\"Unique metric values and their counts saved to {output_csv}\")\n",
    "\n",
    "def filter_csv_files(metric_counts_csv, directory, amount=380000, max_workers=8):\n",
    "    metric_counts = pl.read_csv(metric_counts_csv).filter(pl.col(\"amount\") >= amount)\n",
    "    valid_metrics = set(metric_counts[\"metric\"].to_list())\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "    def process_file(file_name):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        try:\n",
    "            df = pl.read_csv(file_path)\n",
    "            if \"field\" in df.columns:\n",
    "                df_filtered = df.filter(pl.col(\"field\").is_in(valid_metrics))\n",
    "                if len(df_filtered) < 1000:\n",
    "                    os.remove(file_path)\n",
    "                else:\n",
    "                    df_filtered.write_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = [executor.submit(process_file, f) for f in file_list]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Filtering files\"):\n",
    "            pass\n",
    "                \n",
    "def refactor_facts_data(source_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(source_dir, file)\n",
    "            output_file_path = os.path.join(output_dir, file)\n",
    "            \n",
    "            if os.path.exists(output_file_path):\n",
    "                try:\n",
    "                    existing_df = pl.read_csv(output_file_path)\n",
    "                    if 'end' in existing_df.columns and 'field' not in existing_df.columns:\n",
    "                        print(f\"Skipping already processed file: {file}\")\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading existing processed file {file}: {e}. Re-processing...\")\n",
    "            \n",
    "            try:\n",
    "                df = pl.read_csv(file_path)\n",
    "                \n",
    "                if not {'end', 'field', 'val'}.issubset(df.columns):\n",
    "                    print(f\"Skipping {file}: Missing required columns.\")\n",
    "                    continue\n",
    "                \n",
    "                df_pivot = df.pivot(index='end', on='field', values='val', aggregate_function='first')\n",
    "                \n",
    "                df_pivot.write_csv(output_file_path)\n",
    "                # print(f\"Processed: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}. Deleting file...\")\n",
    "                if os.path.exists(output_file_path):\n",
    "                    os.remove(output_file_path)\n",
    "            \n",
    "def download_stock_data(source_dir, target_folder, ticker_df, failed_log=\"failed_tickers.csv\"):\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "    \n",
    "    failed_tickers = []\n",
    "    \n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.startswith(\"CIK\") and file.endswith(\".csv\"):\n",
    "            cik_str = file[3:13]\n",
    "            output_path = os.path.join(target_folder, file)\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                continue\n",
    "\n",
    "            row = ticker_df.filter(ticker_df['cik_str'] == cik_str)\n",
    "            if row.is_empty():\n",
    "                failed_tickers.append([file, \"No matching ticker\"])\n",
    "                continue\n",
    "            \n",
    "            ticker = row['ticker'][0]\n",
    "            \n",
    "            try:\n",
    "                stock_data = yf.download(ticker, start=\"2009-01-01\", end=\"2024-12-31\", interval=\"1wk\")\n",
    "                if stock_data.empty:\n",
    "                    failed_tickers.append([file, \"No data available\"])\n",
    "                    continue\n",
    "                \n",
    "                stock_data.reset_index(inplace=True)\n",
    "\n",
    "                pl_df = pl.from_pandas(stock_data)\n",
    "                pl_df.write_csv(output_path)\n",
    "            except Exception as e:\n",
    "                failed_tickers.append([file, str(e)])\n",
    "    \n",
    "    if failed_tickers:\n",
    "        failed_df = pl.DataFrame(failed_tickers, schema=[\"file_name\", \"Reason\"])\n",
    "        failed_df.write_csv(failed_log)\n",
    "    \n",
    "    print(\"Download complete.\")\n",
    "    \n",
    "def refactor_stock_data(directory_path, max_workers=4):\n",
    "    file_names = [\n",
    "        f for f in os.listdir(directory_path)\n",
    "        if f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    def process_file(file_name):\n",
    "        try:\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            df = pl.read_csv(file_path)\n",
    "\n",
    "            if df.height < 120:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {file_name} (less than 120 entries)\")\n",
    "                return\n",
    "\n",
    "            if df.width < 5:\n",
    "                print(f\"Skipped {file_name} (less than 5 columns)\")\n",
    "                return\n",
    "\n",
    "            df = df.rename({\n",
    "                df.columns[0]: \"date\",\n",
    "                df.columns[1]: \"close\",\n",
    "                df.columns[2]: \"high\",\n",
    "                df.columns[3]: \"low\",\n",
    "                df.columns[4]: \"open\",\n",
    "                df.columns[5]: \"volume\"\n",
    "            })\n",
    "\n",
    "            # Clean up date\n",
    "            df = df.with_columns(df[\"date\"].str.split(\"T\").list.get(0).alias(\"date\"))\n",
    "            df = df.with_columns(df[\"date\"].str.split(\" \").list.get(0).alias(\"date\"))\n",
    "\n",
    "            df.write_csv(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_file, f): f for f in file_names}\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Refactoring stock data\"):\n",
    "            pass\n",
    "            # print(f\"Processed {file_name}\")\n",
    "            \n",
    "def combine_files(facts_dir, stock_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_files = {f for f in os.listdir(facts_dir) if f.endswith(\".csv\")}\n",
    "    stock_files = {f for f in os.listdir(stock_dir) if f.endswith(\".csv\")}\n",
    "    common_files = temp_files.intersection(stock_files)\n",
    "    def process_pair(file):\n",
    "        temp_path = os.path.join(facts_dir, file)\n",
    "        stock_path = os.path.join(stock_dir, file)\n",
    "\n",
    "        df_temp = pl.read_csv(temp_path, try_parse_dates=True)\n",
    "        df_stock = pl.read_csv(stock_path, try_parse_dates=True)\n",
    "\n",
    "        df_merged = df_temp.join(df_stock, left_on=\"end\", right_on=\"date\", how=\"outer\")\n",
    "\n",
    "        df_merged = df_merged.with_columns(\n",
    "            pl.when(df_merged[\"date\"].is_null())\n",
    "            .then(df_merged[\"end\"])\n",
    "            .otherwise(df_merged[\"date\"])\n",
    "            .alias(\"date\")\n",
    "        )\n",
    "\n",
    "        df_merged = df_merged.drop(\"end\")\n",
    "\n",
    "        output_path = os.path.join(output_dir, file)\n",
    "        df_merged.write_csv(output_path)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = {executor.submit(process_pair, file): file for file in common_files}\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Combining files\"):\n",
    "            pass\n",
    "\n",
    "def fill_nulls(source_dir):\n",
    "    def process_file(file_path):\n",
    "        df = pl.read_csv(file_path)\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\")\n",
    "        )\n",
    "\n",
    "        df = df.sort(by=\"date\")\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"volume\").fill_null(0).alias(\"volume\")\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"close\").fill_null(strategy=\"forward\").alias(\"close\"),\n",
    "            pl.col(\"high\").fill_null(strategy=\"forward\").alias(\"high\"),\n",
    "            pl.col(\"low\").fill_null(strategy=\"forward\").alias(\"low\"),\n",
    "            pl.col(\"open\").fill_null(strategy=\"forward\").alias(\"open\"),\n",
    "        )\n",
    "        df = df.with_columns(\n",
    "            df[:, :27].with_columns(pl.all().fill_null(strategy=\"forward\"))\n",
    "        )\n",
    "        df = df.drop_nulls()\n",
    "\n",
    "        df.write_csv(file_path)\n",
    "        print(f\"Processed: {file_name}\")\n",
    "\n",
    "    csv_files = [\n",
    "        os.path.join(source_dir, file_name)\n",
    "        for file_name in os.listdir(source_dir)\n",
    "        if file_name.endswith(\".csv\")\n",
    "    ]\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        executor.map(process_file, csv_files)\n",
    "            \n",
    "def add_company_cik(source_dir):    \n",
    "    for file_name in tqdm(os.listdir(source_dir), desc=\"Processing files\"):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            cik = file_name[3:13]\n",
    "            \n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            \n",
    "            df = pl.read_csv(file_path)\n",
    "            \n",
    "            df = df.with_columns(pl.lit(cik).alias(\"CIK\"))\n",
    "            \n",
    "            df.write_csv(file_path)\n",
    "            \n",
    "            # print(f\"Updated {file_name} with CIK {cik}\")\n",
    "\n",
    "def add_indicators(source_dir):\n",
    "    def process_file(file_path):\n",
    "        df = pl.read_csv(file_path)\n",
    "        if df.width > 6:\n",
    "            return\n",
    "        df = df.to_pandas()\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].astype(np.float64)\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # df['stoch_k'] = ta.stoch(df['high'], df['low'], df['close'], window=14)\n",
    "        # df['parabolic_sar'] = ta.psar(df['high'], df['low'], df['close'])\n",
    "        # ichimoku = ta.ichimoku(df['high'], df['low'], df['close'])\n",
    "        # df['ichimoku_a'] = ichimoku['ichimoku_a']\n",
    "        # df['ichimoku_b'] = ichimoku['ichimoku_b']\n",
    "        # df['ichimoku_base_line'] = ichimoku['ichimoku_base_line']\n",
    "\n",
    "        # SMAs & EMAs\n",
    "        df['SMA_5'] = df['close'].rolling(window=10).mean()  # 2 weeks\n",
    "        df['SMA_15'] = df['close'].rolling(window=15).mean()  # 3 weeks\n",
    "        df[\"EMA_15\"] = ta.ema(df[\"close\"], window=15)  # 3 weeks\n",
    "        df[\"EMA_30\"] = ta.ema(df[\"close\"], window=30)  # 6 weeks\n",
    "\n",
    "        # MACD & ADX\n",
    "        df = df.join(ta.macd(df[\"close\"], window_slow=30, window_fast=15, window_sign=9))\n",
    "        # df = df.join(ta.adx(df[\"high\"], df[\"low\"], df[\"close\"], window=30))  # 6 weeks\n",
    "        # df = df.join(ta.adx(df[\"high\"], df[\"low\"], df[\"close\"], window=50))  # 10 weeks\n",
    "\n",
    "        # RSI\n",
    "        df[\"RSI_14\"] = ta.rsi(df[\"close\"], window=14)  # Standard (2 weeks)\n",
    "        df[\"RSI_21\"] = ta.rsi(df[\"close\"], window=21)  # About 3 weeks\n",
    "\n",
    "        # ROC & CCI\n",
    "        df[\"ROC_21\"] = ta.roc(df[\"close\"], window=21)  # 4 weeks\n",
    "        df[\"ROC_30\"] = ta.roc(df[\"close\"], window=30)  # 6 weeks\n",
    "        df[\"CCI_30\"] = ta.cci(df[\"high\"], df[\"low\"], df[\"close\"], window=30)  # 6 weeks\n",
    "        df[\"CCI_60\"] = ta.cci(df[\"high\"], df[\"low\"], df[\"close\"], window=60)  # 12 weeks\n",
    "\n",
    "        # ATR\n",
    "        df[\"ATR_14\"] = ta.atr(df[\"high\"], df[\"low\"], df[\"close\"], window=14)\n",
    "\n",
    "        # OBV & MFI\n",
    "        df[\"OBV\"] = ta.obv(df[\"close\"], df[\"volume\"])\n",
    "        df[\"MFI_30\"] = ta.mfi(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], window=30)  # 6 weeks\n",
    "        df[\"MFI_60\"] = ta.mfi(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], window=60)  # 12 weeks\n",
    "\n",
    "        df['bb_upper'] = df['SMA_15'] + 2 * df['close'].rolling(window=20).std()\n",
    "        df['bb_lower'] = df['SMA_15'] - 2 * df['close'].rolling(window=20).std()\n",
    "        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['SMA_15']\n",
    "\n",
    "        # Target (weekly return)\n",
    "        df[\"target\"] = (df[\"close\"].shift(-7) / df[\"close\"]) - 1\n",
    "\n",
    "        # Daily return and Rolling statistics\n",
    "        df[\"daily_return\"] = df[\"close\"].pct_change()\n",
    "        df[\"rolling_mean_5\"] = df[\"close\"].rolling(5).mean()\n",
    "        df[\"rolling_std_5\"] = df[\"close\"].rolling(5).std()\n",
    "\n",
    "        # Lagged values\n",
    "        df[\"lag_1\"] = df[\"close\"].shift(1)\n",
    "        df[\"lag_2\"] = df[\"close\"].shift(2)\n",
    "        df[\"lag_return_1\"] = df[\"daily_return\"].shift(1)\n",
    "\n",
    "        # Weekly trend\n",
    "        df[\"weekly_trend\"] = df[\"close\"].shift(0) / df[\"close\"].shift(7) - 1\n",
    "        df = pl.from_pandas(df)\n",
    "\n",
    "        #week/weekday\n",
    "        df = df.sort(\"date\")\n",
    "\n",
    "        df = df.with_columns([\n",
    "            df[\"date\"].dt.week().alias(\"week\"),\n",
    "            df[\"date\"].dt.weekday().alias(\"weekday\")\n",
    "        ])\n",
    "        df.write_csv(file_path)\n",
    "\n",
    "    csv_files = [\n",
    "        os.path.join(source_dir, file_name)\n",
    "        for file_name in os.listdir(source_dir)\n",
    "        if file_name.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        executor.map(process_file, csv_files)"
   ],
   "id": "695496517fd5bb11",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "extract_facts_to_csv(ticker_df, company_facts_dir, facts_dir)\n",
    "count_unique_metrics(facts_dir, \".\\\\unique_metrics.csv\")\n",
    "filter_csv_files(\".\\\\unique_metrics.csv\", facts_dir)"
   ],
   "id": "1b75c50c8c643842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:05:38.594633Z",
     "start_time": "2025-04-10T22:03:29.324061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "count_unique_metrics(facts_dir, \".\\\\unique_metrics.csv\")\n",
    "filter_csv_files(\".\\\\unique_metrics.csv\", facts_dir)"
   ],
   "id": "7b0070084cc9b58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique metric values and their counts saved to .\\unique_metrics.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering files: 100%|██████████| 5347/5347 [00:18<00:00, 288.90it/s]\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "refactor_facts_data(facts_dir, facts_dir)",
   "id": "dac176e661b39e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "download_stock_data(facts_dir, stock_data_dir, ticker_df)\n",
    "refactor_stock_data(stock_data_dir)"
   ],
   "id": "e60dd754462f033d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:09:44.278317Z",
     "start_time": "2025-04-10T22:08:18.686409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "add_indicators(stock_data_dir)\n",
    "refactor_stock_data(stock_data_dir)"
   ],
   "id": "e06fcef9aefa7925",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Refactoring stock data: 100%|██████████| 4192/4192 [00:09<00:00, 433.74it/s]\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:11:15.465803Z",
     "start_time": "2025-04-10T22:09:44.289138Z"
    }
   },
   "cell_type": "code",
   "source": "combine_files(facts_dir, stock_data_dir, company_data_dir)",
   "id": "aba3c703486d8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_26216\\3829910157.py:219: DeprecationWarning: Use of `how='outer'` should be replaced with `how='full'`.\n",
      "  df_merged = df_temp.join(df_stock, left_on=\"end\", right_on=\"date\", how=\"outer\")\n",
      "Combining files: 100%|██████████| 4040/4040 [01:30<00:00, 44.74it/s]\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:13:05.558039Z",
     "start_time": "2025-04-10T22:11:16.745834Z"
    }
   },
   "cell_type": "code",
   "source": "add_company_cik(company_data_dir)",
   "id": "74b877130715207c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 4040/4040 [01:48<00:00, 37.13it/s]\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:14:22.536858Z",
     "start_time": "2025-04-10T22:13:05.594660Z"
    }
   },
   "cell_type": "code",
   "source": "fill_nulls(company_data_dir)",
   "id": "53a3d26abf1e1a60",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:16:37.647535Z",
     "start_time": "2025-04-10T22:16:37.643472Z"
    }
   },
   "cell_type": "code",
   "source": "print(os.getcwd())\n",
   "id": "6ba29a90991594da",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\PycharmProjects\\stock_market_forecasting_LSTM\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:27:31.353291Z",
     "start_time": "2025-04-10T22:18:22.709910Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#done really only for convenience in LSTM part so for now i will keep it as is\n",
    "\n",
    "import glob\n",
    "print(os.getcwd())\n",
    "os.chdir(os.path.join(data_dir, \"company_data\"))\n",
    "print(os.getcwd())\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "\n",
    "merged_df.to_csv(os.path.join(data_dir,\"all_data.csv\"), index=False)"
   ],
   "id": "ed6db971d4ff58f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\PycharmProjects\\stock_market_forecasting_LSTM\n",
      "C:\\Users\\Daniel\\PycharmProjects\\stock_market_forecasting_LSTM\\data\\company_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_26216\\342831638.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T22:27:43.775315Z",
     "start_time": "2025-04-10T22:27:31.432660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_df = pl.read_csv(os.path.join(data_dir, \"all_data.csv\"))\n",
    "print(merged_df.head())\n",
    "print(merged_df.columns)\n",
    "print(merged_df.shape)"
   ],
   "id": "f12eabc863cf426b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 35)\n",
      "┌──────────┬─────────────┬─────────────┬─────────────┬───┬────────┬──────┬────────────┬────────────┐\n",
      "│ Assets   ┆ AssetsCurre ┆ CashAndCash ┆ CommonStock ┆ … ┆ volume ┆ CIK  ┆ IncomeLoss ┆ OperatingI │\n",
      "│ ---      ┆ nt          ┆ Equivalents ┆ ParOrStated ┆   ┆ ---    ┆ ---  ┆ FromContin ┆ ncomeLoss  │\n",
      "│ f64      ┆ ---         ┆ AtCarryi…   ┆ ValuePer…   ┆   ┆ i64    ┆ i64  ┆ uingOperat ┆ ---        │\n",
      "│          ┆ f64         ┆ ---         ┆ ---         ┆   ┆        ┆      ┆ …          ┆ str        │\n",
      "│          ┆             ┆ f64         ┆ f64         ┆   ┆        ┆      ┆ ---        ┆            │\n",
      "│          ┆             ┆             ┆             ┆   ┆        ┆      ┆ str        ┆            │\n",
      "╞══════════╪═════════════╪═════════════╪═════════════╪═══╪════════╪══════╪════════════╪════════════╡\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0         ┆ … ┆ 218100 ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0         ┆ … ┆ 139800 ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0         ┆ … ┆ 180100 ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0         ┆ … ┆ 138900 ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0         ┆ … ┆ 206900 ┆ 1750 ┆ null       ┆ null       │\n",
      "└──────────┴─────────────┴─────────────┴─────────────┴───┴────────┴──────┴────────────┴────────────┘\n",
      "['Assets', 'AssetsCurrent', 'CashAndCashEquivalentsAtCarryingValue', 'CommonStockParOrStatedValuePerShare', 'CommonStockSharesAuthorized', 'CommonStockSharesIssued', 'CommonStockValue', 'ComprehensiveIncomeNetOfTax', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'IncomeTaxExpenseBenefit', 'InterestExpense', 'LiabilitiesAndStockholdersEquity', 'LiabilitiesCurrent', 'NetCashProvidedByUsedInFinancingActivities', 'NetCashProvidedByUsedInInvestingActivities', 'NetCashProvidedByUsedInOperatingActivities', 'NetIncomeLoss', 'PropertyPlantAndEquipmentNet', 'RetainedEarningsAccumulatedDeficit', 'ShareBasedCompensation', 'StockholdersEquity', 'WeightedAverageNumberOfDilutedSharesOutstanding', 'WeightedAverageNumberOfSharesOutstandingBasic', 'Liabilities', 'date', 'close', 'high', 'low', 'open', 'volume', 'CIK', 'IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest', 'OperatingIncomeLoss']\n",
      "(5573542, 35)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7c2f407146fedf5e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
