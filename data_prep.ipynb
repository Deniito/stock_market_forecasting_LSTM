{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-09T12:33:45.594453Z",
     "start_time": "2025-04-09T12:33:45.592023Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import yfinance as yf\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta_remake as ta\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T12:33:46.087235Z",
     "start_time": "2025-04-09T12:33:46.084520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#expected to be a directory with submission, companyfacts, ticker.txt and company_tickers.json all available on SEC website\n",
    "\n",
    "main_dir = \"Output_dir\"\n",
    "temp_data_dir= \"data\"\n",
    "stock_data_dir =\"stock_data\"\n",
    "data_dir = os.path.join(main_dir, \"Data\")\n",
    "submissions_dir = os.path.join(main_dir, \"submission\")\n",
    "company_facts_dir = os.path.join(main_dir, \"companyfacts\")"
   ],
   "id": "d8adb47829773133",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T12:33:46.829281Z",
     "start_time": "2025-04-09T12:33:46.824522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cik_ticker_cvs = os.path.join(main_dir, \"CIK.csv\")\n",
    "\n",
    "ticker_df = pl.read_csv(cik_ticker_cvs).with_columns(\n",
    "    pl.col(\"cik_str\").cast(pl.Utf8).str.zfill(10))"
   ],
   "id": "429da99431c20f2e",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T12:31:36.597868Z",
     "start_time": "2025-04-09T12:31:36.577720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_facts_to_csv(ticker_df, source_dir, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    valid_ciks = set(ticker_df[\"cik_str\"].cast(str))\n",
    "    file_list = [f for f in os.listdir(source_dir) if f.endswith(\".json\")]\n",
    "    \n",
    "    for file_name in tqdm(file_list, desc=\"Processing files\"):\n",
    "        cik_number = file_name.replace(\"CIK\", \"\").split(\".\")[0]\n",
    "        \n",
    "        if cik_number not in valid_ciks:\n",
    "            #print(f\"Skipping {file_name} as its CIK is not in the ticker dataframe.\")\n",
    "            continue\n",
    "        \n",
    "        file_path = os.path.join(source_dir, file_name)\n",
    "        try:\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            output_csv = os.path.join(output_folder, f\"{os.path.splitext(file_name)[0]}.csv\")\n",
    "            \n",
    "            facts = data.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "            rows = []\n",
    "\n",
    "            for field, field_data in facts.items():\n",
    "                units = field_data.get(\"units\", {})\n",
    "                \n",
    "                for unit_type, entries in units.items(): \n",
    "                    for entry in entries:\n",
    "                        entry[\"field\"] = field\n",
    "                        entry[\"unit_type\"] = unit_type\n",
    "                        rows.append(entry)\n",
    "\n",
    "            if rows:\n",
    "                df = pl.DataFrame(rows)\n",
    "                df.write_csv(output_csv)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {file_name}: {e}\")\n",
    "    \n",
    "def count_unique_metrics(source_dir, output_csv=\"unique_metrics.csv\"):\n",
    "    metric_counts = Counter()\n",
    "\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            \n",
    "            try:\n",
    "                df = pl.read_csv(file_path)\n",
    "                if \"field\" in df.columns:\n",
    "                    metrics = df[\"field\"].drop_nulls().to_list()\n",
    "                    metric_counts.update(metrics) \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    output_df = pl.DataFrame({\n",
    "        \"metric\": list(metric_counts.keys()),\n",
    "        \"amount\": list(metric_counts.values())\n",
    "    })\n",
    "\n",
    "\n",
    "    output_df.write_csv(output_csv)\n",
    "\n",
    "    print(f\"Unique metric values and their counts saved to {output_csv}\")\n",
    "\n",
    "def filter_csv_files(metric_counts_csv, directory):\n",
    "    metric_counts = pl.read_csv(metric_counts_csv).filter(pl.col(\"amount\") >= 380000)\n",
    "    \n",
    "    valid_metrics = set(metric_counts[\"metric\"].to_list())\n",
    "\n",
    "    # Process each CSV file\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(directory, file_name)\n",
    "            \n",
    "            try:\n",
    "                df = pl.read_csv(file_path)\n",
    "                \n",
    "                if \"field\" in df.columns:\n",
    "                    df_filtered = df.filter(pl.col(\"field\").is_in(valid_metrics))\n",
    "                    \n",
    "                    if len(df_filtered) < 1000:\n",
    "                        os.remove(file_path) \n",
    "                        print(f\"Deleted {file_name} (fewer than 1000 rows after filtering).\")\n",
    "                    else:\n",
    "                        df_filtered.write_csv(file_path)\n",
    "                        print(f\"Filtered {file_name}: {len(df)} -> {len(df_filtered)} rows\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "                \n",
    "def refactor_facts_data(source_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(source_dir, file)\n",
    "            output_file_path = os.path.join(output_dir, file)\n",
    "            \n",
    "            if os.path.exists(output_file_path):\n",
    "                try:\n",
    "                    existing_df = pl.read_csv(output_file_path)\n",
    "                    if 'end' in existing_df.columns and 'field' not in existing_df.columns:\n",
    "                        print(f\"Skipping already processed file: {file}\")\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading existing processed file {file}: {e}. Re-processing...\")\n",
    "            \n",
    "            try:\n",
    "                df = pl.read_csv(file_path)\n",
    "                \n",
    "                if not {'end', 'field', 'val'}.issubset(df.columns):\n",
    "                    print(f\"Skipping {file}: Missing required columns.\")\n",
    "                    continue\n",
    "                \n",
    "                df_pivot = df.pivot(index='end', on='field', values='val', aggregate_function='first')\n",
    "                \n",
    "                df_pivot.write_csv(output_file_path)\n",
    "                print(f\"Processed: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}. Deleting file...\")\n",
    "                if os.path.exists(output_file_path):\n",
    "                    os.remove(output_file_path)\n",
    "            \n",
    "def download_stock_data(source_dir, target_folder, ticker_df, failed_log=\"failed_tickers.csv\"):\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "    \n",
    "    failed_tickers = []\n",
    "    \n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.startswith(\"CIK\") and file.endswith(\".csv\"):\n",
    "            cik_str = file[3:13]\n",
    "            output_path = os.path.join(target_folder, file)\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                continue\n",
    "\n",
    "            row = ticker_df.filter(ticker_df['cik_str'] == cik_str)\n",
    "            if row.is_empty():\n",
    "                failed_tickers.append([file, \"No matching ticker\"])\n",
    "                continue\n",
    "            \n",
    "            ticker = row['ticker'][0]\n",
    "            \n",
    "            try:\n",
    "                stock_data = yf.download(ticker, start=\"2009-01-01\", end=\"2024-12-31\", interval=\"1wk\")\n",
    "                if stock_data.empty:\n",
    "                    failed_tickers.append([file, \"No data available\"])\n",
    "                    continue\n",
    "                \n",
    "                stock_data.reset_index(inplace=True)\n",
    "\n",
    "                pl_df = pl.from_pandas(stock_data)\n",
    "                pl_df.write_csv(output_path)\n",
    "            except Exception as e:\n",
    "                failed_tickers.append([file, str(e)])\n",
    "    \n",
    "    if failed_tickers:\n",
    "        failed_df = pl.DataFrame(failed_tickers, schema=[\"file_name\", \"Reason\"])\n",
    "        failed_df.write_csv(failed_log)\n",
    "    \n",
    "    print(\"Download complete.\")\n",
    "    \n",
    "def refactor_stock_data(directory_path, max_workers=4):\n",
    "    file_names = [\n",
    "        f for f in os.listdir(directory_path)\n",
    "        if f.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    def process_file(file_name):\n",
    "        try:\n",
    "            file_path = os.path.join(directory_path, file_name)\n",
    "            df = pl.read_csv(file_path)\n",
    "\n",
    "            if df.height < 120:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {file_name} (less than 120 entries)\")\n",
    "                return\n",
    "\n",
    "            if df.width < 5:\n",
    "                print(f\"Skipped {file_name} (less than 5 columns)\")\n",
    "                return\n",
    "\n",
    "            df = df.rename({\n",
    "                df.columns[0]: \"date\",\n",
    "                df.columns[1]: \"close\",\n",
    "                df.columns[2]: \"high\",\n",
    "                df.columns[3]: \"low\",\n",
    "                df.columns[4]: \"open\",\n",
    "                df.columns[5]: \"volume\"\n",
    "            })\n",
    "\n",
    "            # Clean up date\n",
    "            df = df.with_columns(df[\"date\"].str.split(\"T\").list.get(0).alias(\"date\"))\n",
    "            df = df.with_columns(df[\"date\"].str.split(\" \").list.get(0).alias(\"date\"))\n",
    "\n",
    "            df.write_csv(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_file, f): f for f in file_names}\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Refactoring stock data\"):\n",
    "            pass\n",
    "            # print(f\"Processed {file_name}\")\n",
    "            \n",
    "def combine_files(temp_dir, stock_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_files = {f for f in os.listdir(temp_dir) if f.endswith(\".csv\")}\n",
    "    stock_files = {f for f in os.listdir(stock_dir) if f.endswith(\".csv\")}\n",
    "    common_files = temp_files.intersection(stock_files)\n",
    "    def process_pair(file):\n",
    "        temp_path = os.path.join(temp_dir, file)\n",
    "        stock_path = os.path.join(stock_dir, file)\n",
    "\n",
    "        df_temp = pl.read_csv(temp_path, try_parse_dates=True)\n",
    "        df_stock = pl.read_csv(stock_path, try_parse_dates=True)\n",
    "\n",
    "        df_merged = df_temp.join(df_stock, left_on=\"end\", right_on=\"date\", how=\"outer\")\n",
    "\n",
    "        df_merged = df_merged.with_columns(\n",
    "            pl.when(df_merged[\"date\"].is_null())\n",
    "            .then(df_merged[\"end\"])\n",
    "            .otherwise(df_merged[\"date\"])\n",
    "            .alias(\"date\")\n",
    "        )\n",
    "\n",
    "        df_merged = df_merged.drop(\"end\")\n",
    "\n",
    "        output_path = os.path.join(output_dir, file)\n",
    "        df_merged.write_csv(output_path)\n",
    "        # print(f\"Processed: {file}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = {executor.submit(process_pair, file): file for file in common_files}\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Combining files\"):\n",
    "            pass\n",
    "\n",
    "def fill_nulls(source_dir):\n",
    "    def process_file(file_path):\n",
    "        df = pl.read_csv(file_path)\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\")\n",
    "        )\n",
    "\n",
    "        df = df.sort(by=\"date\")\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"volume\").fill_null(0).alias(\"volume\")\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"close\").fill_null(strategy=\"forward\").alias(\"close\"),\n",
    "            pl.col(\"high\").fill_null(strategy=\"forward\").alias(\"high\"),\n",
    "            pl.col(\"low\").fill_null(strategy=\"forward\").alias(\"low\"),\n",
    "            pl.col(\"open\").fill_null(strategy=\"forward\").alias(\"open\"),\n",
    "        )\n",
    "        df = df.with_columns(\n",
    "            df[:, :27].with_columns(pl.all().fill_null(strategy=\"forward\"))\n",
    "        )\n",
    "        df = df.drop_nulls()\n",
    "\n",
    "        df.write_csv(file_path)\n",
    "        print(f\"Processed: {file_name}\")\n",
    "\n",
    "    csv_files = [\n",
    "        os.path.join(source_dir, file_name)\n",
    "        for file_name in os.listdir(source_dir)\n",
    "        if file_name.endswith(\".csv\")\n",
    "    ]\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        executor.map(process_file, csv_files)\n",
    "            \n",
    "def add_company_cik(source_dir):    \n",
    "    for file_name in tqdm(os.listdir(source_dir), desc=\"Processing files\"):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            cik = file_name[3:13]\n",
    "            \n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            \n",
    "            df = pl.read_csv(file_path)\n",
    "            \n",
    "            df = df.with_columns(pl.lit(cik).alias(\"CIK\"))\n",
    "            \n",
    "            df.write_csv(file_path)\n",
    "            \n",
    "            # print(f\"Updated {file_name} with CIK {cik}\")\n",
    "\n",
    "def add_indicators(source_dir):  # you can tweak `max_workers` if needed\n",
    "    def process_file(file_path):\n",
    "        df = pl.read_csv(file_path)\n",
    "        if df.width > 6:\n",
    "            return\n",
    "        df = df.to_pandas()\n",
    "\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        df[numeric_cols] = df[numeric_cols].astype(np.float64)\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # df['stoch_k'] = ta.stoch(df['high'], df['low'], df['close'], window=14)\n",
    "        # df['parabolic_sar'] = ta.psar(df['high'], df['low'], df['close'])\n",
    "        # ichimoku = ta.ichimoku(df['high'], df['low'], df['close'])\n",
    "        # df['ichimoku_a'] = ichimoku['ichimoku_a']\n",
    "        # df['ichimoku_b'] = ichimoku['ichimoku_b']\n",
    "        # df['ichimoku_base_line'] = ichimoku['ichimoku_base_line']\n",
    "\n",
    "        # SMAs & EMAs\n",
    "        df['SMA_5'] = df['close'].rolling(window=10).mean()  # 2 weeks\n",
    "        df['SMA_15'] = df['close'].rolling(window=15).mean()  # 3 weeks\n",
    "        df[\"EMA_15\"] = ta.ema(df[\"close\"], window=15)  # 3 weeks\n",
    "        df[\"EMA_30\"] = ta.ema(df[\"close\"], window=30)  # 6 weeks\n",
    "\n",
    "        # MACD & ADX\n",
    "        df = df.join(ta.macd(df[\"close\"], window_slow=30, window_fast=15, window_sign=9))\n",
    "        # df = df.join(ta.adx(df[\"high\"], df[\"low\"], df[\"close\"], window=30))  # 6 weeks\n",
    "        # df = df.join(ta.adx(df[\"high\"], df[\"low\"], df[\"close\"], window=50))  # 10 weeks\n",
    "\n",
    "        # RSI\n",
    "        df[\"RSI_14\"] = ta.rsi(df[\"close\"], window=14)  # Standard (2 weeks)\n",
    "        df[\"RSI_21\"] = ta.rsi(df[\"close\"], window=21)  # About 3 weeks\n",
    "\n",
    "        # ROC & CCI\n",
    "        df[\"ROC_21\"] = ta.roc(df[\"close\"], window=21)  # 4 weeks\n",
    "        df[\"ROC_30\"] = ta.roc(df[\"close\"], window=30)  # 6 weeks\n",
    "        df[\"CCI_30\"] = ta.cci(df[\"high\"], df[\"low\"], df[\"close\"], window=30)  # 6 weeks\n",
    "        df[\"CCI_60\"] = ta.cci(df[\"high\"], df[\"low\"], df[\"close\"], window=60)  # 12 weeks\n",
    "\n",
    "        # ATR\n",
    "        df[\"ATR_14\"] = ta.atr(df[\"high\"], df[\"low\"], df[\"close\"], window=14)\n",
    "\n",
    "        # OBV & MFI\n",
    "        df[\"OBV\"] = ta.obv(df[\"close\"], df[\"volume\"])\n",
    "        df[\"MFI_30\"] = ta.mfi(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], window=30)  # 6 weeks\n",
    "        df[\"MFI_60\"] = ta.mfi(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], window=60)  # 12 weeks\n",
    "\n",
    "        df['bb_upper'] = df['SMA_15'] + 2 * df['close'].rolling(window=20).std()\n",
    "        df['bb_lower'] = df['SMA_15'] - 2 * df['close'].rolling(window=20).std()\n",
    "        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['SMA_15']\n",
    "\n",
    "        # Target (weekly return)\n",
    "        df[\"target\"] = (df[\"close\"].shift(-7) / df[\"close\"]) - 1\n",
    "\n",
    "        # Daily return and Rolling statistics\n",
    "        df[\"daily_return\"] = df[\"close\"].pct_change()\n",
    "        df[\"rolling_mean_5\"] = df[\"close\"].rolling(5).mean()\n",
    "        df[\"rolling_std_5\"] = df[\"close\"].rolling(5).std()\n",
    "\n",
    "        # Lagged values\n",
    "        df[\"lag_1\"] = df[\"close\"].shift(1)\n",
    "        df[\"lag_2\"] = df[\"close\"].shift(2)\n",
    "        df[\"lag_return_1\"] = df[\"daily_return\"].shift(1)\n",
    "\n",
    "        # Weekly trend\n",
    "        df[\"weekly_trend\"] = df[\"close\"].shift(0) / df[\"close\"].shift(7) - 1\n",
    "        df = pl.from_pandas(df)\n",
    "\n",
    "        #week/weekday\n",
    "        df = df.sort(\"date\")\n",
    "\n",
    "        df = df.with_columns([\n",
    "            df[\"date\"].dt.week().alias(\"week\"),\n",
    "            df[\"date\"].dt.weekday().alias(\"weekday\")\n",
    "        ])\n",
    "        df.write_csv(file_path)\n",
    "\n",
    "    csv_files = [\n",
    "        os.path.join(source_dir, file_name)\n",
    "        for file_name in os.listdir(source_dir)\n",
    "        if file_name.endswith(\".csv\")\n",
    "    ]\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        executor.map(process_file, csv_files)\n",
    "\n",
    "def change_to_weekly(source_dir):\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            df = pl.read_csv(file_path)\n",
    "            \n",
    "\n",
    "            \n",
    "            weekly_stock = df.group_by([\"year\", \"week\"]).agg([\n",
    "                pl.col(\"close\").mean().alias(\"close\"),\n",
    "                pl.col(\"high\").mean().alias(\"high\"),\n",
    "                pl.col(\"low\").mean().alias(\"low\"),\n",
    "                pl.col(\"open\").mean().alias(\"open\"),\n",
    "                pl.col(\"volume\").sum().alias(\"volume\")\n",
    "            ])\n",
    "\n",
    "            df_friday_indicator = df.filter(df[\"weekday\"] == 5).drop([\"weekday\", \"low\", \"high\", \"close\", \"open\", \"volume\"])\n",
    "            weekly_df = df_friday_indicator.join(weekly_stock, on=[\"year\", \"week\"], how=\"left\")\n",
    "            print(f\"Chnaged {file_name}\")\n",
    "            weekly_df.write_csv(file_path)"
   ],
   "id": "695496517fd5bb11",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "extract_facts_to_csv(ticker_df, company_facts_dir, temp_data_dir)\n",
    "count_unique_metrics(temp_data_dir, \".\\\\unique_metrics.csv\")\n",
    "filter_csv_files(\".\\\\unique_metrics.csv\", temp_data_dir)"
   ],
   "id": "1b75c50c8c643842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "count_unique_metrics(temp_data_dir, \".\\\\unique_metrics.csv\")\n",
    "filter_csv_files(\".\\\\unique_metrics.csv\", temp_data_dir)"
   ],
   "id": "7b0070084cc9b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "refactor_facts_data(temp_data_dir, temp_data_dir)",
   "id": "dac176e661b39e60",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "download_stock_data(temp_data_dir, stock_data_dir, ticker_df)\n",
    "refactor_stock_data(stock_data_dir)"
   ],
   "id": "e60dd754462f033d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "add_indicators(stock_data_dir)\n",
    "refactor_stock_data(stock_data_dir)"
   ],
   "id": "e06fcef9aefa7925",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# change_to_weekly(stock_data_dir)",
   "id": "94d4a9a5be043b0f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T10:39:09.533436Z",
     "start_time": "2025-04-09T10:38:52.031428Z"
    }
   },
   "cell_type": "code",
   "source": "combine_files(temp_data_dir, stock_data_dir, data_dir)",
   "id": "aba3c703486d8d",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_332962/117941233.py:219: DeprecationWarning: Use of `how='outer'` should be replaced with `how='full'`.\n",
      "  df_merged = df_temp.join(df_stock, left_on=\"end\", right_on=\"date\", how=\"outer\")\n",
      "Combining files: 100%|██████████| 4192/4192 [00:17<00:00, 241.14it/s]\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "add_company_cik(data_dir)",
   "id": "74b877130715207c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T10:55:55.432769Z",
     "start_time": "2025-04-09T10:55:46.564339Z"
    }
   },
   "cell_type": "code",
   "source": "fill_nulls(data_dir)",
   "id": "53a3d26abf1e1a60",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T12:35:18.541517Z",
     "start_time": "2025-04-09T12:33:53.437478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import glob\n",
    "print(os.getcwd())\n",
    "os.chdir(os.path.join(main_dir, \"Data\"))\n",
    "\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "print(os.getcwd())\n",
    "print(os.path.join(main_dir,\"all_data.csv\"))\n",
    "merged_df.to_csv(os.path.join(main_dir,\"all_data.csv\"), index=False)"
   ],
   "id": "ed6db971d4ff58f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/PycharmProjects/TFT for market prediction\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13906/2957703503.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/daniel/PycharmProjects/TFT for market prediction\n",
      "Output_dir/all_data.csv\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T12:35:29.859437Z",
     "start_time": "2025-04-09T12:35:28.726019Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_df = pl.read_csv(os.path.join(main_dir, \"all_data.csv\"))\n",
    "print(merged_df.head())\n",
    "print(merged_df.columns)\n",
    "print(merged_df.shape)"
   ],
   "id": "f12eabc863cf426b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 70)\n",
      "┌───────────┬─────────────┬─────────────┬─────────────┬───┬──────┬─────────┬─────────┬─────────────┐\n",
      "│ Assets    ┆ AssetsCurre ┆ CashAndCash ┆ CommonStock ┆ … ┆ week ┆ weekday ┆ CIK     ┆ GeneralAndA │\n",
      "│ ---       ┆ nt          ┆ Equivalents ┆ ParOrStated ┆   ┆ ---  ┆ ---     ┆ ---     ┆ dministrati │\n",
      "│ f64       ┆ ---         ┆ AtCarryi…   ┆ ValuePer…   ┆   ┆ i64  ┆ i64     ┆ i64     ┆ veExpens…   │\n",
      "│           ┆ f64         ┆ ---         ┆ ---         ┆   ┆      ┆         ┆         ┆ ---         │\n",
      "│           ┆             ┆ f64         ┆ f64         ┆   ┆      ┆         ┆         ┆ f64         │\n",
      "╞═══════════╪═════════════╪═════════════╪═════════════╪═══╪══════╪═════════╪═════════╪═════════════╡\n",
      "│ 7.5475e9  ┆ 2.9988e9    ┆ 1.4260e9    ┆ 0.0         ┆ … ┆ 40   ┆ 3       ┆ 1318605 ┆ null        │\n",
      "│ 8.0925e9  ┆ 2.7916e9    ┆ 1.1969e9    ┆ 0.0         ┆ … ┆ 53   ┆ 4       ┆ 1318605 ┆ null        │\n",
      "│ 1.2592e10 ┆ 5.1724e9    ┆ 3.0843e9    ┆ 0.0         ┆ … ┆ 39   ┆ 5       ┆ 1318605 ┆ null        │\n",
      "│ 2.5054e10 ┆ 7.0279e9    ┆ 4.0066e9    ┆ 0.0         ┆ … ┆ 13   ┆ 5       ┆ 1318605 ┆ null        │\n",
      "│ 2.6044e10 ┆ 6.3594e9    ┆ 3.0359e9    ┆ 0.0         ┆ … ┆ 26   ┆ 5       ┆ 1318605 ┆ null        │\n",
      "└───────────┴─────────────┴─────────────┴─────────────┴───┴──────┴─────────┴─────────┴─────────────┘\n",
      "['Assets', 'AssetsCurrent', 'CashAndCashEquivalentsAtCarryingValue', 'CommonStockParOrStatedValuePerShare', 'CommonStockSharesAuthorized', 'CommonStockSharesIssued', 'CommonStockSharesOutstanding', 'CommonStockValue', 'ComprehensiveIncomeNetOfTax', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest', 'IncomeTaxExpenseBenefit', 'InterestExpense', 'Liabilities', 'LiabilitiesAndStockholdersEquity', 'LiabilitiesCurrent', 'NetCashProvidedByUsedInFinancingActivities', 'NetCashProvidedByUsedInInvestingActivities', 'NetCashProvidedByUsedInOperatingActivities', 'NetIncomeLoss', 'OperatingExpenses', 'OperatingIncomeLoss', 'ProfitLoss', 'PropertyPlantAndEquipmentNet', 'RetainedEarningsAccumulatedDeficit', 'Revenues', 'ShareBasedCompensation', 'StockholdersEquity', 'WeightedAverageNumberOfDilutedSharesOutstanding', 'WeightedAverageNumberOfSharesOutstandingBasic', 'date', 'close', 'high', 'low', 'open', 'volume', 'SMA_5', 'SMA_15', 'EMA_15', 'EMA_30', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'RSI_14', 'RSI_21', 'ROC_21', 'ROC_30', 'CCI_30', 'CCI_60', 'ATR_14', 'OBV', 'MFI_30', 'MFI_60', 'bb_upper', 'bb_lower', 'bb_width', 'target', 'daily_return', 'rolling_mean_5', 'rolling_std_5', 'lag_1', 'lag_2', 'lag_return_1', 'weekly_trend', 'week', 'weekday', 'CIK', 'GeneralAndAdministrativeExpense']\n",
      "(1580837, 70)\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T20:55:58.692795Z",
     "start_time": "2025-04-08T20:55:58.688345Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = {\n",
    "    \"close\": [100, 102, 101, 105, 107]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add the daily_diff column\n",
    "df[\"daily_diff\"] = df[\"close\"].shift(-1) / df[\"close\"]\n",
    "\n",
    "print(df)"
   ],
   "id": "f5039ab067926d8d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   close  daily_diff  daily_return_pct\n",
      "0    100    1.020000          2.000000\n",
      "1    102    0.990196         -0.980392\n",
      "2    101    1.039604          3.960396\n",
      "3    105    1.019048          1.904762\n",
      "4    107         NaN               NaN\n"
     ]
    }
   ],
   "execution_count": 38
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
