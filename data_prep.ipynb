{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-13T10:46:48.933689Z",
     "start_time": "2025-04-13T10:46:42.488491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import json\n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "import yfinance as yf\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_ta_remake as ta\n",
    "from pathlib import Path\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:46:48.976420Z",
     "start_time": "2025-04-13T10:46:48.965868Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#expected to be a directory with submission, companyfacts, ticker.txt and company_tickers.json all available on SEC website\n",
    "data_dir = \"data\"\n",
    "\n",
    "filing_data_dir = os.path.join(data_dir, \"filing_data\")\n",
    "stock_data_dir = os.path.join(data_dir, \"stock_data\")\n",
    "\n",
    "company_data_dir = os.path.join(data_dir, \"company_data\")\n",
    "\n",
    "submissions_dir = os.path.join(data_dir, \"submissions\")\n",
    "company_facts_dir = os.path.join(data_dir, \"companyfacts\")"
   ],
   "id": "aa04c73991d1b157",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:46:49.292327Z",
     "start_time": "2025-04-13T10:46:48.990029Z"
    }
   },
   "cell_type": "code",
   "source": [
    "cik_ticker_cvs = os.path.join(data_dir, \"CIK.csv\")\n",
    "\n",
    "ticker_df = pl.read_csv(cik_ticker_cvs).with_columns(\n",
    "    pl.col(\"cik_str\").cast(pl.Utf8).str.zfill(10))"
   ],
   "id": "429da99431c20f2e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T10:46:49.730486Z",
     "start_time": "2025-04-13T10:46:49.567219Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_facts_to_csv(ticker_df, source_dir, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    valid_ciks = set(ticker_df[\"cik_str\"].cast(str))\n",
    "    file_list = [f for f in os.listdir(source_dir) if f.endswith(\".json\")]\n",
    "    file_paths = [os.path.join(source_dir, f) for f in file_list]\n",
    "\n",
    "    def process_file(file_path):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        cik_number = file_name.replace(\"CIK\", \"\").split(\".\")[0]\n",
    "\n",
    "        if cik_number not in valid_ciks:\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(file_path, \"r\") as file:\n",
    "                data = json.load(file)\n",
    "\n",
    "            output_csv = os.path.join(output_folder, f\"{os.path.splitext(file_name)[0]}.csv\")\n",
    "\n",
    "            facts = data.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "            rows = []\n",
    "\n",
    "            for field, field_data in facts.items():\n",
    "                units = field_data.get(\"units\", {})\n",
    "                for unit_type, entries in units.items():\n",
    "                    for entry in entries:\n",
    "                        entry[\"field\"] = field\n",
    "                        entry[\"unit_type\"] = unit_type\n",
    "                        rows.append(entry)\n",
    "\n",
    "            if rows:\n",
    "                df = pl.DataFrame(rows)\n",
    "                df.write_csv(output_csv)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "            \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_file, path) for path in file_paths]\n",
    "        for _ in tqdm(concurrent.futures.as_completed(futures), total=len(futures), desc=\"Processing files\"):\n",
    "            pass\n",
    "    \n",
    "def count_unique_metrics(source_dir, output_csv=\"unique_metrics.csv\"):\n",
    "    metric_counts = Counter()\n",
    "    for file_name in os.listdir(source_dir):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            \n",
    "            try:\n",
    "                df = pl.read_csv(file_path)\n",
    "                if \"field\" in df.columns:\n",
    "                    metrics = df[\"field\"].drop_nulls().to_list()\n",
    "                    metric_counts.update(metrics) \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    output_df = pl.DataFrame({\n",
    "        \"metric\": list(metric_counts.keys()),\n",
    "        \"amount\": list(metric_counts.values())\n",
    "    })\n",
    "    output_df.write_csv(output_csv)\n",
    "    print(f\"Unique metric values and their counts saved to {output_csv}\")\n",
    "\n",
    "def filter_csv_files(metric_counts_csv, directory, amount=380000):\n",
    "    metric_counts = pl.read_csv(metric_counts_csv).filter(pl.col(\"amount\") >= amount)\n",
    "    valid_metrics = set(metric_counts[\"metric\"].to_list())\n",
    "    file_list = [f for f in os.listdir(directory) if f.endswith(\".csv\")]\n",
    "\n",
    "    def process_file(file_name):\n",
    "        file_path = os.path.join(directory, file_name)\n",
    "        try:\n",
    "            df = pl.read_csv(file_path)\n",
    "            if \"field\" in df.columns:\n",
    "                df_filtered = df.filter(pl.col(\"field\").is_in(valid_metrics))\n",
    "                if len(df_filtered) < 1000:\n",
    "                    os.remove(file_path)\n",
    "                else:\n",
    "                    df_filtered.write_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_file, f) for f in file_list]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Filtering files\"):\n",
    "            pass\n",
    "                \n",
    "def refactor_facts_data(source_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.endswith(\".csv\"):\n",
    "            file_path = os.path.join(source_dir, file)\n",
    "            output_file_path = os.path.join(output_dir, file)\n",
    "            \n",
    "            if os.path.exists(output_file_path):\n",
    "                try:\n",
    "                    existing_df = pl.read_csv(output_file_path)\n",
    "                    if 'end' in existing_df.columns and 'field' not in existing_df.columns:\n",
    "                        print(f\"Skipping already processed file: {file}\")\n",
    "                        continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading existing processed file {file}: {e}. Re-processing...\")\n",
    "            \n",
    "            try:\n",
    "                df = pl.read_csv(file_path)\n",
    "                \n",
    "                if not {'end', 'field', 'val'}.issubset(df.columns):\n",
    "                    print(f\"Skipping {file}: Missing required columns.\")\n",
    "                    continue\n",
    "                \n",
    "                df_pivot = df.pivot(index='end', on='field', values='val', aggregate_function='first')\n",
    "                \n",
    "                df_pivot.write_csv(output_file_path)\n",
    "                # print(f\"Processed: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}. Deleting file...\")\n",
    "                if os.path.exists(output_file_path):\n",
    "                    os.remove(output_file_path)\n",
    "            \n",
    "def download_stock_data(source_dir, target_folder, ticker_df, failed_log=\"failed_tickers.csv\"):\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "    \n",
    "    failed_tickers = []\n",
    "    \n",
    "    for file in os.listdir(source_dir):\n",
    "        if file.startswith(\"CIK\") and file.endswith(\".csv\"):\n",
    "            cik_str = file[3:13]\n",
    "            output_path = os.path.join(target_folder, file)\n",
    "            \n",
    "            if os.path.exists(output_path):\n",
    "                continue\n",
    "\n",
    "            row = ticker_df.filter(ticker_df['cik_str'] == cik_str)\n",
    "            if row.is_empty():\n",
    "                failed_tickers.append([file, \"No matching ticker\"])\n",
    "                continue\n",
    "            \n",
    "            ticker = row['ticker'][0]\n",
    "            \n",
    "            try:\n",
    "                stock_data = yf.download(ticker, start=\"2009-01-01\", end=\"2024-12-31\", interval=\"1wk\")\n",
    "                if stock_data.empty:\n",
    "                    failed_tickers.append([file, \"No data available\"])\n",
    "                    continue\n",
    "                \n",
    "                stock_data.reset_index(inplace=True)\n",
    "\n",
    "                pl_df = pl.from_pandas(stock_data)\n",
    "                pl_df.write_csv(output_path)\n",
    "            except Exception as e:\n",
    "                failed_tickers.append([file, str(e)])\n",
    "    \n",
    "    if failed_tickers:\n",
    "        failed_df = pl.DataFrame(failed_tickers, schema=[\"file_name\", \"Reason\"])\n",
    "        failed_df.write_csv(failed_log)\n",
    "    \n",
    "    print(\"Download complete.\")\n",
    "    \n",
    "def refactor_stock_data(source_dir, max_workers=4):\n",
    "    file_list = [f for f in os.listdir(source_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "    def process_file(file_name):\n",
    "        try:\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "            df = pl.read_csv(file_path)\n",
    "\n",
    "            if df.height < 120:\n",
    "                os.remove(file_path)\n",
    "                print(f\"Deleted {file_name} (less than 120 entries)\")\n",
    "                return\n",
    "\n",
    "            if df.width < 5:\n",
    "                print(f\"Skipped {file_name} (less than 5 columns)\")\n",
    "                return\n",
    "\n",
    "            df = df.rename({\n",
    "                df.columns[0]: \"date\",\n",
    "                df.columns[1]: \"close\",\n",
    "                df.columns[2]: \"high\",\n",
    "                df.columns[3]: \"low\",\n",
    "                df.columns[4]: \"open\",\n",
    "                df.columns[5]: \"volume\"\n",
    "            })\n",
    "\n",
    "            # Clean up date\n",
    "            df = df.with_columns(df[\"date\"].str.split(\"T\").list.get(0).alias(\"date\"))\n",
    "            df = df.with_columns(df[\"date\"].str.split(\" \").list.get(0).alias(\"date\"))\n",
    "\n",
    "            df.write_csv(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_file, f): f for f in source_dir}\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Refactoring stock data\"):\n",
    "            pass\n",
    "            # print(f\"Processed {file_name}\")\n",
    "            \n",
    "def combine_files(facts_dir, stock_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    temp_files = {f for f in os.listdir(facts_dir) if f.endswith(\".csv\")}\n",
    "    stock_files = {f for f in os.listdir(stock_dir) if f.endswith(\".csv\")}\n",
    "    common_files = temp_files.intersection(stock_files)\n",
    "    def process_pair(file):\n",
    "        temp_path = os.path.join(facts_dir, file)\n",
    "        stock_path = os.path.join(stock_dir, file)\n",
    "\n",
    "        df_temp = pl.read_csv(temp_path, try_parse_dates=True)\n",
    "        df_stock = pl.read_csv(stock_path, try_parse_dates=True)\n",
    "\n",
    "        df_merged = df_temp.join(df_stock, left_on=\"end\", right_on=\"date\", how=\"outer\")\n",
    "\n",
    "        df_merged = df_merged.with_columns(\n",
    "            pl.when(df_merged[\"date\"].is_null())\n",
    "            .then(df_merged[\"end\"])\n",
    "            .otherwise(df_merged[\"date\"])\n",
    "            .alias(\"date\")\n",
    "        )\n",
    "\n",
    "        df_merged = df_merged.drop(\"end\")\n",
    "\n",
    "        output_path = os.path.join(output_dir, file)\n",
    "        df_merged.write_csv(output_path)\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = {executor.submit(process_pair, file): file for file in common_files}\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Combining files\"):\n",
    "            pass\n",
    "\n",
    "def fill_nulls(source_dir):\n",
    "    def process_file(file_path):\n",
    "        file_path = os.path.join(source_dir, file_path)\n",
    "        df = pl.read_csv(file_path)\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"date\").str.strptime(pl.Date, \"%Y-%m-%d\").alias(\"date\")\n",
    "        )\n",
    "\n",
    "        df = df.sort(by=\"date\")\n",
    "\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"volume\").fill_null(0).alias(\"volume\")\n",
    "        )\n",
    "\n",
    "        df = df.with_columns(pl.all().fill_null(strategy=\"forward\"))\n",
    "        \n",
    "        df = df.drop_nulls()\n",
    "\n",
    "        df.write_csv(file_path)\n",
    "        print(f\"Processed: {file_name}\")\n",
    "\n",
    "    file_list = [f for f in os.listdir(source_dir) if f.endswith(\".csv\")]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_file, f) for f in file_list]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Adding CIK\"):\n",
    "            pass\n",
    "            \n",
    "def add_company_cik(source_dir):\n",
    "    file_list = [f for f in os.listdir(source_dir) if f.endswith(\".csv\")]\n",
    "\n",
    "    def process_file(file_name):\n",
    "        try:\n",
    "            cik = file_name[3:13]\n",
    "            file_path = os.path.join(source_dir, file_name)\n",
    "\n",
    "            df = pl.read_csv(file_path)\n",
    "            df = df.with_columns(pl.lit(cik).alias(\"CIK\"))\n",
    "            df.write_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_file, f) for f in file_list]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Adding CIK\"):\n",
    "            pass\n",
    "\n",
    "def add_indicators(source_dir):\n",
    "    def process_file(file_path):\n",
    "        \n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        if df.shape[1] > 7:\n",
    "            # print(\"width > 7\")\n",
    "            return\n",
    "        \n",
    "        # numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        # df[numeric_cols] = df[numeric_cols].astype(np.float64)\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        \n",
    "        # df['stoch_k'] = ta.stoch(df['high'], df['low'], df['close'], window=14)\n",
    "        # df['parabolic_sar'] = ta.psar(df['high'], df['low'], df['close'])\n",
    "        \n",
    "        ichimoku = ta.ichimoku(\n",
    "            high=df['high'],\n",
    "            low=df['low'],\n",
    "            close=df['close'],\n",
    "            window1=9,\n",
    "            window2=26,\n",
    "            window3=52\n",
    "        )\n",
    "        \n",
    "        # Extracting the Ichimoku components\n",
    "        ichimoku_df = ichimoku[0]\n",
    "        \n",
    "        # Adding Ichimoku components to the dataframe\n",
    "        df['ichimoku_a'] = ichimoku_df['ISA_9']\n",
    "        df['ichimoku_b'] = ichimoku_df['ISB_26']\n",
    "        df['ichimoku_base_line'] = ichimoku_df['IKS_26']\n",
    "        \n",
    "        # SMAs & EMAs\n",
    "        df['SMA_30'] = df['close'].rolling(window=30).mean()\n",
    "        df['SMA_60'] = df['close'].rolling(window=60).mean()\n",
    "        df[\"EMA_60\"] = ta.ema(df[\"close\"], window=60)\n",
    "        df[\"EMA_90\"] = ta.ema(df[\"close\"], window=90)\n",
    "        \n",
    "        # MACD & ADX\n",
    "        df = df.join(ta.macd(df[\"close\"], window_slow=60, window_fast=30, window_sign=15))\n",
    "        \n",
    "        # RSI\n",
    "        df[\"RSI_60\"] = ta.rsi(df[\"close\"], window=60)\n",
    "        df[\"RSI_90\"] = ta.rsi(df[\"close\"], window=90)\n",
    "        \n",
    "        # ROC & CCI\n",
    "        df[\"ROC_60\"] = ta.roc(df[\"close\"], window=60)\n",
    "        df[\"ROC_90\"] = ta.roc(df[\"close\"], window=90)\n",
    "        df[\"CCI_60\"] = ta.cci(df[\"high\"], df[\"low\"], df[\"close\"], window=60)\n",
    "        df[\"CCI_90\"] = ta.cci(df[\"high\"], df[\"low\"], df[\"close\"], window=90)\n",
    "        \n",
    "        # ATR\n",
    "        df[\"ATR_30\"] = ta.atr(df[\"high\"], df[\"low\"], df[\"close\"], window=30)\n",
    "        \n",
    "        # OBV & MFI\n",
    "        df[\"OBV\"] = ta.obv(df[\"close\"], df[\"volume\"])\n",
    "        df[\"MFI_60\"] = ta.mfi(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], window=60)\n",
    "        df[\"MFI_90\"] = ta.mfi(df[\"high\"], df[\"low\"], df[\"close\"], df[\"volume\"], window=90)\n",
    "        \n",
    "        df['bb_upper'] = df['SMA_60'] + 2 * df['close'].rolling(window=60).std()\n",
    "        df['bb_lower'] = df['SMA_60'] - 2 * df['close'].rolling(window=60).std()\n",
    "        df['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['SMA_60']\n",
    "        \n",
    "        # Target\n",
    "        df[\"target\"] = (df[\"close\"].shift(-90) / df[\"close\"]) - 1\n",
    "        \n",
    "        # Daily return and Rolling statistics\n",
    "        df[\"daily_return\"] = df[\"close\"].pct_change()\n",
    "        df[\"rolling_mean_90\"] = df[\"close\"].rolling(90).mean()\n",
    "        df[\"rolling_std_90\"] = df[\"close\"].rolling(90).std()\n",
    "        \n",
    "        # Weekly trend\n",
    "        df[\"weekly_trend\"] = df[\"close\"].shift(0) / df[\"close\"].shift(7) - 1\n",
    "        \n",
    "        #week/weekday\n",
    "        df = df.sort_values(\"date\")\n",
    "        \n",
    "        df[\"week\"] = df[\"date\"].dt.isocalendar().week\n",
    "        df[\"weekday\"] = df[\"date\"].dt.weekday\n",
    "    \n",
    "        df.to_csv(file_path,  index=False)\n",
    "\n",
    "    file_list = [f for f in os.listdir(source_dir) if f.endswith(\".csv\")]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:\n",
    "        futures = [executor.submit(process_file, os.path.join(source_dir, f)) for f in file_list]\n",
    "        for _ in tqdm(as_completed(futures), total=len(futures), desc=\"Adding indicators\"):\n",
    "            pass"
   ],
   "id": "695496517fd5bb11",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "extract_facts_to_csv(ticker_df, company_facts_dir, filing_data_dir)\n",
    "count_unique_metrics(filing_data_dir, \".\\\\unique_metrics.csv\")\n",
    "filter_csv_files(\".\\\\unique_metrics.csv\", filing_data_dir)"
   ],
   "id": "1b75c50c8c643842",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "count_unique_metrics(filing_data_dir, \".\\\\unique_metrics.csv\")\n",
    "filter_csv_files(\".\\\\unique_metrics.csv\", filing_data_dir)"
   ],
   "id": "7b0070084cc9b58",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-12T13:07:24.670106Z",
     "start_time": "2025-04-12T13:06:57.519122Z"
    }
   },
   "cell_type": "code",
   "source": "refactor_facts_data(filing_data_dir, filing_data_dir)",
   "id": "dac176e661b39e60",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "download_stock_data(filing_data_dir, stock_data_dir, ticker_df)\n",
    "refactor_stock_data(stock_data_dir)"
   ],
   "id": "e60dd754462f033d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "add_indicators(stock_data_dir)\n",
    "refactor_stock_data(stock_data_dir)"
   ],
   "id": "e06fcef9aefa7925",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "combine_files(filing_data_dir, stock_data_dir, company_data_dir)",
   "id": "aba3c703486d8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T00:01:51.170410Z",
     "start_time": "2025-04-13T00:00:43.185413Z"
    }
   },
   "cell_type": "code",
   "source": "add_company_cik(company_data_dir)",
   "id": "74b877130715207c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding CIK: 100%|██████████| 4077/4077 [01:07<00:00, 60.53it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T00:21:08.937921Z",
     "start_time": "2025-04-13T00:19:52.950104Z"
    }
   },
   "cell_type": "code",
   "source": "fill_nulls(company_data_dir)",
   "id": "53a3d26abf1e1a60",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding CIK: 100%|██████████| 4077/4077 [01:04<00:00, 63.06it/s]\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "#done really only for convenience in LSTM part so for now i will keep it as is\n",
    "csv_files = [\n",
    "    os.path.join(company_data_dir, file_name)\n",
    "    for file_name in os.listdir(company_data_dir)\n",
    "    if file_name.endswith(\".csv\")\n",
    "]\n",
    "\n",
    "dataframes = []\n",
    "for file in tqdm(csv_files, desc=\"Loading CSVs\"):\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True, sort=False)\n",
    "\n",
    "merged_df.to_csv(os.path.join(data_dir,\"all_data.csv\"), index=False)"
   ],
   "id": "ed6db971d4ff58f7",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-13T11:11:42.774635Z",
     "start_time": "2025-04-13T11:11:12.266154Z"
    }
   },
   "cell_type": "code",
   "source": [
    "merged_df = pl.read_csv(os.path.join(data_dir, \"all_data.csv\"))\n",
    "print(merged_df.head())\n",
    "print(merged_df.columns)\n",
    "print(merged_df.shape)"
   ],
   "id": "f12eabc863cf426b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (5, 65)\n",
      "┌──────────┬─────────────┬─────────────┬────────────┬───┬─────────┬──────┬────────────┬────────────┐\n",
      "│ Assets   ┆ AssetsCurre ┆ CashAndCash ┆ CommonStoc ┆ … ┆ weekday ┆ CIK  ┆ IncomeLoss ┆ OperatingI │\n",
      "│ ---      ┆ nt          ┆ Equivalents ┆ kParOrStat ┆   ┆ ---     ┆ ---  ┆ FromContin ┆ ncomeLoss  │\n",
      "│ f64      ┆ ---         ┆ AtCarryi…   ┆ edValuePer ┆   ┆ i64     ┆ i64  ┆ uingOperat ┆ ---        │\n",
      "│          ┆ f64         ┆ ---         ┆ …          ┆   ┆         ┆      ┆ …          ┆ str        │\n",
      "│          ┆             ┆ f64         ┆ ---        ┆   ┆         ┆      ┆ ---        ┆            │\n",
      "│          ┆             ┆             ┆ f64        ┆   ┆         ┆      ┆ str        ┆            │\n",
      "╞══════════╪═════════════╪═════════════╪════════════╪═══╪═════════╪══════╪════════════╪════════════╡\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0        ┆ … ┆ 1       ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0        ┆ … ┆ 2       ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0        ┆ … ┆ 3       ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0        ┆ … ┆ 4       ┆ 1750 ┆ null       ┆ null       │\n",
      "│ 1.6733e9 ┆ 1.0678e9    ┆ 5.27e7      ┆ 1.0        ┆ … ┆ 0       ┆ 1750 ┆ null       ┆ null       │\n",
      "└──────────┴─────────────┴─────────────┴────────────┴───┴─────────┴──────┴────────────┴────────────┘\n",
      "['Assets', 'AssetsCurrent', 'CashAndCashEquivalentsAtCarryingValue', 'CommonStockParOrStatedValuePerShare', 'CommonStockSharesAuthorized', 'CommonStockSharesIssued', 'CommonStockValue', 'ComprehensiveIncomeNetOfTax', 'EarningsPerShareBasic', 'EarningsPerShareDiluted', 'GrossProfit', 'IncomeTaxExpenseBenefit', 'InterestExpense', 'LiabilitiesAndStockholdersEquity', 'LiabilitiesCurrent', 'NetCashProvidedByUsedInFinancingActivities', 'NetCashProvidedByUsedInInvestingActivities', 'NetCashProvidedByUsedInOperatingActivities', 'NetIncomeLoss', 'PropertyPlantAndEquipmentNet', 'RetainedEarningsAccumulatedDeficit', 'ShareBasedCompensation', 'StockholdersEquity', 'WeightedAverageNumberOfDilutedSharesOutstanding', 'WeightedAverageNumberOfSharesOutstandingBasic', 'Liabilities', 'date', 'close', 'high', 'low', 'open', 'volume', 'ichimoku_a', 'ichimoku_b', 'ichimoku_base_line', 'SMA_30', 'SMA_60', 'EMA_60', 'EMA_90', 'MACD_12_26_9', 'MACDh_12_26_9', 'MACDs_12_26_9', 'RSI_60', 'RSI_90', 'ROC_60', 'ROC_90', 'CCI_60', 'CCI_90', 'ATR_30', 'OBV', 'MFI_60', 'MFI_90', 'bb_upper', 'bb_lower', 'bb_width', 'target', 'daily_return', 'rolling_mean_90', 'rolling_std_90', 'weekly_trend', 'week', 'weekday', 'CIK', 'IncomeLossFromContinuingOperationsBeforeIncomeTaxesExtraordinaryItemsNoncontrollingInterest', 'OperatingIncomeLoss']\n",
      "(6883485, 65)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "89662f20bcac2ee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
